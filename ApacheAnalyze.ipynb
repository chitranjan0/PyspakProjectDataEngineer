{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ace5dd3-9ea9-4b23-859e-bdbd7f9b8605",
   "metadata": {},
   "source": [
    "# Sample Apache Spark Notebook\n",
    "\n",
    "Here's an example of cleaning and analyzing an Apache access log, but this time within an interactive Notebook environment!\n",
    "\n",
    "While prototyping data engineering solutions, notebook environments are popular.\n",
    "\n",
    "If you installed the pyspark package or are working within an already-established environment for Spark, things will probably \"just work.\" But if not, using the findspark package will tie the notebook to your existing Spark installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "910ebf75-0a78-4b7f-9abc-d8fdd0093093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e663604-b27d-4b02-bade-5e11f46a0a4a",
   "metadata": {},
   "source": [
    "One nice thing about notebooks is that you can leave little comments and explanations like this, in markdown format.\n",
    "\n",
    "We'll start by importing the stuff we need, and creating a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944a1921-c630-4280-90b0-5d662480bd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, col, count, desc\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"ApacheLogAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986bfe8e-97cc-4520-b2ed-8066bfeab7e4",
   "metadata": {},
   "source": [
    "Next we'll load up our sample access log, and load in the raw text into a Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34f70e75-94d2-4fc4-9362-dc5d9cd6135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log file path (Update this path to your log file location)\n",
    "log_file = \"./access_log.txt\"\n",
    "\n",
    "# Read log file as text\n",
    "logs_df = spark.read.text(log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2fefad-fb71-49eb-8eda-a2482d7579b3",
   "metadata": {},
   "source": [
    "We'll now parse the log into the fields we are interested in. Note, we know there is some bad data in here where the status code is empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ece337f-2f61-49b1-8b72-56cdec462fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular expression pattern to extract fields\n",
    "log_pattern = r'(\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\S{3}) (\\d+) \"(.*?)\"'\n",
    "\n",
    "# Extract fields using regex\n",
    "parsed_logs_df = logs_df.select(\n",
    "    regexp_extract('value', log_pattern, 1).alias(\"ip_address\"),\n",
    "    regexp_extract('value', log_pattern, 2).alias(\"timestamp\"),\n",
    "    regexp_extract('value', log_pattern, 3).alias(\"request\"),\n",
    "    regexp_extract('value', log_pattern, 4).alias(\"status\"),\n",
    "    regexp_extract('value', log_pattern, 5).cast(\"integer\").alias(\"bytes\"),\n",
    "    regexp_extract('value', log_pattern, 6).alias(\"user_agent\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c6194c-6691-41c4-987b-19ec9d2c792f",
   "metadata": {},
   "source": [
    "We'll use a filter to just remove those bogus rows with no status, and cast the remaining status codes to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f99c34fb-fc1b-47e1-8ef9-d699f2664ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with empty status fields\n",
    "cleaned_df = parsed_logs_df.filter(col(\"status\").isNotNull() & (col(\"status\") != \"\"))\n",
    "cleaned_df = cleaned_df.withColumn(\"status\", col(\"status\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b06dcd-a1d8-4af9-800f-672cd50c41f3",
   "metadata": {},
   "source": [
    "A nice thing about notebooks is you can break up your processing into these separate blocks, and inspect the output just for whatever it is you're doing. Then you can go back and iterate on that piece of code as needed, rather than re-running everything.\n",
    "\n",
    "Let's further process our data to split out the method and endpoint from the request field, and then preview the resulting Dataframe we have thusfar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b860a8f-977b-4018-b36c-a9378ad93acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+----------------------------------+------+-----+----------------------------------+------+---------------------+\n",
      "|ip_address    |timestamp                 |request                           |status|bytes|user_agent                        |method|endpoint             |\n",
      "+--------------+--------------------------+----------------------------------+------+-----+----------------------------------+------+---------------------+\n",
      "|66.249.75.159 |29/Nov/2015:03:50:05 +0000|GET /robots.txt HTTP/1.1          |200   |55   |-                                 |GET   |/robots.txt          |\n",
      "|66.249.75.168 |29/Nov/2015:03:50:06 +0000|GET /blog/ HTTP/1.1               |200   |8083 |-                                 |GET   |/blog/               |\n",
      "|185.71.216.232|29/Nov/2015:03:53:15 +0000|POST /wp-login.php HTTP/1.1       |200   |1691 |http://nohatenews.com/wp-login.php|POST  |/wp-login.php        |\n",
      "|54.165.199.171|29/Nov/2015:04:32:27 +0000|GET /sitemap_index.xml HTTP/1.0   |200   |592  |-                                 |GET   |/sitemap_index.xml   |\n",
      "|54.165.199.171|29/Nov/2015:04:32:27 +0000|GET /post-sitemap.xml HTTP/1.0    |200   |2502 |-                                 |GET   |/post-sitemap.xml    |\n",
      "|54.165.199.171|29/Nov/2015:04:32:27 +0000|GET /page-sitemap.xml HTTP/1.0    |200   |11462|-                                 |GET   |/page-sitemap.xml    |\n",
      "|54.165.199.171|29/Nov/2015:04:32:27 +0000|GET /category-sitemap.xml HTTP/1.0|200   |585  |-                                 |GET   |/category-sitemap.xml|\n",
      "|54.165.199.171|29/Nov/2015:04:32:27 +0000|GET /blog/ HTTP/1.0               |200   |31746|-                                 |GET   |/blog/               |\n",
      "|54.165.199.171|29/Nov/2015:04:32:27 +0000|GET /orlando-sports/ HTTP/1.0     |200   |35510|-                                 |GET   |/orlando-sports/     |\n",
      "|54.165.199.171|29/Nov/2015:04:32:37 +0000|GET /about/ HTTP/1.0              |200   |25121|-                                 |GET   |/about/              |\n",
      "+--------------+--------------------------+----------------------------------+------+-----+----------------------------------+------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split request field to get HTTP method and endpoint\n",
    "parsed_logs_df = cleaned_df.withColumn(\"method\", regexp_extract(\"request\", r'(\\S+)', 1)) \\\n",
    "                               .withColumn(\"endpoint\", regexp_extract(\"request\", r' (\\S+) ', 1))\n",
    "\n",
    "# Show parsed log data\n",
    "parsed_logs_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae1b2c-bb61-4e38-ad39-aba7c619da18",
   "metadata": {},
   "source": [
    "Now let's start doing some analysis. We'll start with displaying the top 10 IP addresses. Unfortunately, as usual, there's a hacker trying to DOS me:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35ced343-cde5-4c6c-a835-f08a6fc7cacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=======================================>                   (2 + 1) / 3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|    ip_address|request_count|\n",
      "+--------------+-------------+\n",
      "| 46.166.139.20|        68487|\n",
      "|54.165.199.171|         4381|\n",
      "|195.154.250.88|         1723|\n",
      "| 97.100.169.53|          240|\n",
      "|              |          212|\n",
      "| 62.210.88.201|           81|\n",
      "|  66.249.66.59|           69|\n",
      "|  66.249.66.62|           54|\n",
      "|   66.249.66.3|           47|\n",
      "|   52.91.1.103|           39|\n",
      "+--------------+-------------+\n",
      "only showing top 10 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 1. Count requests per IP address\n",
    "ip_count_df = parsed_logs_df.groupBy(\"ip_address\").agg(count(\"*\").alias(\"request_count\")).orderBy(desc(\"request_count\"))\n",
    "ip_count_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16b7b59-49e9-43c4-98a2-5972e3a9d442",
   "metadata": {},
   "source": [
    "Let's look at the top endpoints, and right away we can see that our friend is trying to break into my WordPress site through xmlrpc.php vulnerabilities and trying to brute-force their way in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b592ffaf-cec4-4101-babe-4f631f12b908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{\"ts\": \"2025-09-30 18:37:10.967\", \"level\": \"ERROR\", \"logger\": \"DataFrameQueryContextLogger\", \"msg\": \"[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `endpoint` cannot be resolved. Did you mean one of the following? [`bytes`, `request`, `status`, `user_agent`, `ip_address`]. SQLSTATE: 42703\", \"context\": {\"file\": \"java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\", \"line\": \"\", \"fragment\": \"col\", \"errorClass\": \"UNRESOLVED_COLUMN.WITH_SUGGESTION\"}, \"exception\": {\"class\": \"Py4JJavaError\", \"msg\": \"An error occurred while calling o111.agg.\\n: org.apache.spark.sql.AnalysisException: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `endpoint` cannot be resolved. Did you mean one of the following? [`bytes`, `request`, `status`, `user_agent`, `ip_address`]. SQLSTATE: 42703;\\n'Aggregate ['endpoint], ['endpoint, count(1) AS endpoint_count#34L]\\n+- Project [regexp_extract(value#0, (\\\\S+) - - \\\\[(.*?)\\\\] \\\"(.*?)\\\" (\\\\S{3}) (\\\\d+) \\\"(.*?)\\\", 1) AS ip_address#1, regexp_extract(value#0, (\\\\S+) - - \\\\[(.*?)\\\\] \\\"(.*?)\\\" (\\\\S{3}) (\\\\d+) \\\"(.*?)\\\", 2) AS timestamp#2, regexp_extract(value#0, (\\\\S+) - - \\\\[(.*?)\\\\] \\\"(.*?)\\\" (\\\\S{3}) (\\\\d+) \\\"(.*?)\\\", 3) AS request#3, regexp_extract(value#0, (\\\\S+) - - \\\\[(.*?)\\\\] \\\"(.*?)\\\" (\\\\S{3}) (\\\\d+) \\\"(.*?)\\\", 4) AS status#4, cast(regexp_extract(value#0, (\\\\S+) - - \\\\[(.*?)\\\\] \\\"(.*?)\\\" (\\\\S{3}) (\\\\d+) \\\"(.*?)\\\", 5) as int) AS bytes#5, regexp_extract(value#0, (\\\\S+) - - \\\\[(.*?)\\\\] \\\"(.*?)\\\" (\\\\S{3}) (\\\\d+) \\\"(.*?)\\\", 6) AS user_agent#6]\\n   +- Relation [value#0] text\\n\\n\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\tat scala.util.Try$.apply(Try.scala:217)\\n\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\\n\\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\\n\\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\\n\\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\\n\\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\\n\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:84)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:54)\\n\\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:158)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:147)\\n\\tat org.apache.spark.sql.classic.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:54)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\\n\\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\\n\\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\\n\\tat java.base/java.lang.Thread.run(Thread.java:840)\\n\\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\\n\\t\\tat org.apache.spark.sql.errors.QueryCompilationErrors$.unresolvedAttributeError(QueryCompilationErrors.scala:401)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$$failUnresolvedAttribute(CheckAnalysis.scala:169)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7(CheckAnalysis.scala:404)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$7$adapted(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$6$adapted(CheckAnalysis.scala:402)\\n\\t\\tat scala.collection.immutable.List.foreach(List.scala:334)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:402)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:252)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:284)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:255)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:244)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:231)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:299)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:192)\\n\\t\\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:192)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\\n\\t\\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\\n\\t\\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\\n\\t\\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\\n\\t\\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\\n\\t\\tat scala.util.Try$.apply(Try.scala:217)\\n\\t\\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\\n\\t\\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\\n\\t\\t... 23 more\\n\", \"stacktrace\": [{\"class\": null, \"method\": \"deco\", \"file\": \"/opt/homebrew/opt/apache-spark/libexec/python/pyspark/errors/exceptions/captured.py\", \"line\": \"282\"}, {\"class\": null, \"method\": \"get_return_value\", \"file\": \"/opt/homebrew/opt/apache-spark/libexec/python/lib/py4j-0.10.9.9-src.zip/py4j/protocol.py\", \"line\": \"327\"}]}}\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `endpoint` cannot be resolved. Did you mean one of the following? [`bytes`, `request`, `status`, `user_agent`, `ip_address`]. SQLSTATE: 42703;\n'Aggregate ['endpoint], ['endpoint, count(1) AS endpoint_count#34L]\n+- Project [regexp_extract(value#0, (\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\S{3}) (\\d+) \"(.*?)\", 1) AS ip_address#1, regexp_extract(value#0, (\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\S{3}) (\\d+) \"(.*?)\", 2) AS timestamp#2, regexp_extract(value#0, (\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\S{3}) (\\d+) \"(.*?)\", 3) AS request#3, regexp_extract(value#0, (\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\S{3}) (\\d+) \"(.*?)\", 4) AS status#4, cast(regexp_extract(value#0, (\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\S{3}) (\\d+) \"(.*?)\", 5) as int) AS bytes#5, regexp_extract(value#0, (\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\S{3}) (\\d+) \"(.*?)\", 6) AS user_agent#6]\n   +- Relation [value#0] text\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 2. Most requested endpoints\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m endpoint_count_df \u001b[38;5;241m=\u001b[39m \u001b[43mparsed_logs_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mendpoint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mendpoint_count\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39morderBy(desc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mendpoint_count\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m endpoint_count_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/opt/apache-spark/libexec/python/pyspark/sql/group.py:190\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    189\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m cast(Tuple[Column, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], exprs)\n\u001b[0;32m--> 190\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[0;32m/opt/homebrew/opt/apache-spark/libexec/python/lib/py4j-0.10.9.9-src.zip/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/opt/apache-spark/libexec/python/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column, variable, or function parameter with name `endpoint` cannot be resolved. Did you mean one of the following? [`bytes`, `request`, `status`, `user_agent`, `ip_address`]. SQLSTATE: 42703;\n'Aggregate ['endpoint], ['endpoint, count(1) AS endpoint_count#34L]\n+- Project [regexp_extract(value#0, (\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\S{3}) (\\d+) \"(.*?)\", 1) AS ip_address#1, regexp_extract(value#0, (\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\S{3}) (\\d+) \"(.*?)\", 2) AS timestamp#2, regexp_extract(value#0, (\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\S{3}) (\\d+) \"(.*?)\", 3) AS request#3, regexp_extract(value#0, (\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\S{3}) (\\d+) \"(.*?)\", 4) AS status#4, cast(regexp_extract(value#0, (\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\S{3}) (\\d+) \"(.*?)\", 5) as int) AS bytes#5, regexp_extract(value#0, (\\S+) - - \\[(.*?)\\] \"(.*?)\" (\\S{3}) (\\d+) \"(.*?)\", 6) AS user_agent#6]\n   +- Relation [value#0] text\n"
     ]
    }
   ],
   "source": [
    "# 2. Most requested endpoints\n",
    "endpoint_count_df = parsed_logs_df.groupBy(\"endpoint\").agg(count(\"*\").alias(\"endpoint_count\")).orderBy(desc(\"endpoint_count\"))\n",
    "endpoint_count_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3703ce2-66df-4237-a0f8-89fc55a00e05",
   "metadata": {},
   "source": [
    "Let's also take a look at the top status codes. Looks like they've succeeded in making my site unstable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a849fd5e-a529-4688-9201-d93300e4e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+\n",
      "|status|status_count|\n",
      "+------+------------+\n",
      "|   200|       64965|\n",
      "|   500|       10714|\n",
      "|      |         212|\n",
      "|   301|         159|\n",
      "|   404|          24|\n",
      "|   400|           2|\n",
      "|   302|           2|\n",
      "|   405|           1|\n",
      "+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. HTTP status code distribution\n",
    "status_count_df = parsed_logs_df.groupBy(\"status\").agg(count(\"*\").alias(\"status_count\")).orderBy(desc(\"status_count\"))\n",
    "status_count_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af1fcdd-dd63-477f-96c3-3512a4baa67b",
   "metadata": {},
   "source": [
    "Finally we'll shut things down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caa05da8-3c09-49fd-91fe-824e647dd075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6310b049-80f6-47b2-950a-d5b60dad2c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
